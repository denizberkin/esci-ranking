{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GroupShuffleSplit, GroupKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, ndcg_score\n",
    "import lightgbm as lgbm\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# XXX: use smoothed labels?, use scaled scoring?\n",
    "SCORE_MAP = {\"Exact\": 3,  # exact\n",
    "           \"Substitute\": 2,  # substitute\n",
    "           \"Complement\": 1,  # complementary\n",
    "           \"Irrelevant\": 0}  # irrelevant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt: str) -> str:\n",
    "    if txt == \"\":\n",
    "        return txt\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r\"[^a-z0-9\\s]\", \"\", txt)  # remove punc\n",
    "    return txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work on test samples\n",
    "cols = [\"query\", \"product_title\", \"product_description\", \"product_brand\", \"product_color\"]  # product_text is already a combined version\n",
    "df_folder = \"formatted_esci\"\n",
    "train_filenames = [f for f in os.listdir(df_folder) if f.startswith(\"train\")]\n",
    "test_filenames = [f for f in os.listdir(df_folder) if f.startswith(\"test\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filenames: str) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for f in filenames:\n",
    "        df = pd.read_parquet(f\"{df_folder}/{f}\")\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2027874, 652490)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_df(train_filenames[:])\n",
    "df_test = load_df(test_filenames[:])\n",
    "\n",
    "len(df[df.columns[0]]), len(df_test[df_test.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols:\n",
    "    df[col] = df[col].fillna(\"\").apply(preprocess_text)\n",
    "\n",
    "df[\"combined\"] = df[cols].apply(lambda r: \" \".join(r), axis=1)\n",
    "df[\"labels\"] = df[\"esci_label\"].map(SCORE_MAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature ext.\n",
    "\n",
    "# 1. either tfidf\n",
    "vec = TfidfVectorizer(max_features=8000)\n",
    "x = vec.fit_transform(df[\"combined\"])\n",
    "\n",
    "# 2. or transformer-based embedding\n",
    "# transformer = SentenceTransformer(\"CHOOSE ONE HERE\")\n",
    "# x = transformer.encode(df[\"combined\"].tolist(), convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "folds = list(group_kfold.split(x, df[\"labels\"], df[\"query_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_sizes(df_subset):\n",
    "    \"\"\" lgbm requires this \"\"\"\n",
    "    return df_subset.groupby('query_id').size().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027874"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = folds[0][1]\n",
    "\n",
    "train_idx = np.concatenate(folds[1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "x_train = x[train_idx]\n",
    "x_val = x[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_train = get_group_sizes(train_df)\n",
    "groups_val = get_group_sizes(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train_set = lgbm.Dataset(x_train, train_df[\"labels\"], group=groups_train)\n",
    "lgbm_val_set = lgbm.Dataset(x_val, val_df[\"labels\"], group=groups_val)\n",
    "\n",
    "lgbm_model_params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_ndcg(preds, eval_data):\n",
    "    return \"ndcg\", ndcg_score(eval_data.get_label(), preds), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbm.train(lgbm_model_params, lgbm_train_set, valid_sets=[lgbm_val_set], num_boost_round=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(df, preds, query_group_col=\"query_id\"):\n",
    "    scores = []\n",
    "    for qid, group in df.groupby(query_group_col):\n",
    "        t = group[\"labels\"].values.reshape(1, -1)\n",
    "        p = preds[group.index].reshape(1, -1)\n",
    "        \n",
    "        # single result?\n",
    "        if t.shape[1] > 1:\n",
    "            scores.append(ndcg_score(t, p, k=3))\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_val, model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.023205346216754758,\n",
       " 0.0722107022834135,\n",
       " 0.013996284619595916,\n",
       " 0.108182176983583)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_f1 = f1_score(val_df[\"labels\"], np.round(preds), average=\"macro\")\n",
    "test_acc = accuracy_score(val_df[\"labels\"], np.round(preds))\n",
    "test_precision = precision_score(val_df[\"labels\"], np.round(preds), average=\"macro\")\n",
    "test_recall = recall_score(val_df[\"labels\"], np.round(preds), average=\"macro\")\n",
    "\n",
    "test_f1, test_acc, test_precision, test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8935062465710224\n"
     ]
    }
   ],
   "source": [
    "test_ndcg = calculate_ndcg(val_df, preds)\n",
    "print(test_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
